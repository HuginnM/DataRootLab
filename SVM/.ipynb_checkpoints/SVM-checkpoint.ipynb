{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6AD6qcnKCGJ_"
   },
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Welcome to your next lab! You will build SVM algorithm and explore working of this model with different kernels.\n",
    "\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a learning algorithm with OOP in mind:\n",
    "    - Helper functions:\n",
    "        - Kernels\n",
    "        - Kernels matrix\n",
    "        - Computing lagrange multipliers\n",
    "        - Extracting support features\n",
    "        \n",
    "    - Main Model Class:\n",
    "        - Training\n",
    "        - Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Before the start ##\n",
    "\n",
    "SVM is a rather complex method compared to the ones we learned before. For better understanding of the steps in lab we strongly recommend you to get through this topics (links to related guides):\n",
    "\n",
    "- [Lagrangian](https://www.youtube.com/watch?v=BSKtQcLQLWU&list=PLCg2-CTYVrQvNGLbd-FN70UxWZSeKP4wV&index=3&ab_channel=KhanAcademy)\n",
    "- [SVM algorithm and kernels intuition](https://www.youtube.com/watch?v=_PwhiWxHK8o&ab_channel=MITOpenCourseWare)\n",
    "- [Quadratic optimization problem](https://www.youtube.com/watch?v=GZb9647X8sg&ab_channel=KodyPowell)\n",
    "- [Using of cvxopt](https://www.youtube.com/watch?v=_EgoDT7RHwE&ab_channel=AhmadBazzi)\n",
    "- [cvxopt tutorial](https://cvxopt.org/examples/tutorial/qp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fq_EilhNCGKA"
   },
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [cvxopt](http://cvxopt.org) is a software package for convex optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mcO6FkOpCGKC"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a4a877a9257d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'show_progress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cvxopt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxopt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cvxopt.solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qxKCR9sCGKF"
   },
   "source": [
    "## 2 - Overview of the Dataset  ##\n",
    "\n",
    "**Problem Statement**: You are given a dataset  containing:\n",
    "    - a training set of m_train examples\n",
    "    - a test set of m_test examples\n",
    "    - each example is of shape (number of features, 1)\n",
    "    \n",
    "  This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for Poisonous Oak and Ivy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_g_O-5jmCGKF"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X = np.genfromtxt('mush_features.csv')\n",
    "    Y = np.genfromtxt('mush_labels.csv')\n",
    "    \n",
    "    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    train_set_x = train_set_x[:300].astype(float)\n",
    "    train_set_y = train_set_y[:300].astype(float)\n",
    "    \n",
    "    test_set_x = test_set_x[:100].astype(float)\n",
    "    test_set_y = test_set_y[:100].astype(float)\n",
    "    \n",
    "    x_test = train_set_x[:5]\n",
    "    y_test = train_set_y[:5]   \n",
    "    \n",
    "    train_set_x = train_set_x.reshape(train_set_x.shape[0], -1).T\n",
    "    test_set_x = test_set_x.reshape(test_set_x.shape[0], -1).T\n",
    "    \n",
    "    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n",
    "    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n",
    "    \n",
    "    x_test = x_test.reshape(x_test.shape[0], -1).T\n",
    "    y_test = y_test.reshape((1, y_test.shape[0]))\n",
    "    \n",
    "    return train_set_x, test_set_x, train_set_y, test_set_y, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTETwuB0CGKI"
   },
   "source": [
    "Many software bugs in machine learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\n",
    "\n",
    "So, let's check shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9pbP4st-CGKJ"
   },
   "outputs": [],
   "source": [
    "train_set_x, test_set_x, train_set_y, test_set_y, x_test, y_test = load_data()\n",
    "print('train_set_x.shape: ', train_set_x.shape)\n",
    "print('test_set_x.shape: ', test_set_x.shape)\n",
    "print('train_set_y.shape: ', train_set_y.shape)\n",
    "print('test_set_y.shape: ', test_set_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V58fhdj2CGKL"
   },
   "source": [
    "**Expected Output for m_train, m_test**: \n",
    "<table style=\"width:30%\">\n",
    "  <tr>\n",
    "      <td><b>train_set_x.shape:</b></td>\n",
    "    <td> (22, 300) </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><b>test_set_x.shape:</b></td>\n",
    "    <td> (22, 100) </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><b>train_set_y.shape:</b></td>\n",
    "    <td> (1,300) </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><b>test_set_y.shape:</b></td>\n",
    "    <td> (1,100) </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WkSQzJgFCGKM"
   },
   "source": [
    "Distribution of samples in train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6L4ActVBCGKN"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.hist(train_set_y.T)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4isdygxzCGKP"
   },
   "source": [
    "Distribution of samples in test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_hfyRUPRCGKQ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.hist(test_set_y.T)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwMM05R-CGKT"
   },
   "source": [
    "## 3 - General Architecture of the learning algorithm ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BVOA877CGKU"
   },
   "source": [
    "The SVM algorithm is implemented in practice using a kernel.\n",
    "\n",
    "The learning of the hyperplane in linear SVM is done by transforming the problem using some linear algebra.\n",
    "\n",
    "A powerful insight is that the linear SVM can be rephrased using the inner product of any two given observations, rather than the observations themselves. The inner product between two vectors is the sum of the multiplication of each pair of input values.\n",
    "\n",
    "For example, the inner product of the vectors [2, 3] and [5, 6] is 28.\n",
    "\n",
    "The equation for making a prediction for a new input using the dot product between the input ($x$) and each support vector ($x_s$) is calculated as follows: $$f(x) = sign(b + \\sum_{s \\in S}(\\bar{w}_s* k(x, x_s)))$$\n",
    "\n",
    "This is an equation that involves calculating the inner products of a new input vector ($x$) with all support vectors in training data. The coefficients $b$ and $\\bar{w}_s$ (for each input) must be estimated from the training data by the learning algorithm. $S$ is a set of all support vectors and $k()$ is a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ymp4asNcCGKU"
   },
   "source": [
    "### 3.1 - Kernels ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4VIrD-uzCGKV"
   },
   "source": [
    "#### Linear Kernel\n",
    "\n",
    "The dot-product is called the linear kernel and can be re-written as:\n",
    "\n",
    "$$k(x_{i},x _{j}) = x_{i}x_{j}\\tag{1}$$\n",
    "\n",
    "Kernel is the function, that defines the similarity or a distance measure between new data and the support vectors. The dot product is the similarity measure used for linear SVM or a linear kernel because the distance is a linear combination of the inputs.\n",
    "\n",
    "It is desirable to use more complex kernels as it allows lines to separate the classes that are curved or even more complex. This in turn can lead to more accurate classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWmhG4F4CGKW"
   },
   "source": [
    "#### Polynomial Kernel SVM ####\n",
    "Instead of the dot-product, we can use a polynomial kernel, for example:\n",
    "\n",
    "$$k(x_{i},x _{j}) = (x_{i}x_{j} + coef)^{d}\\tag{2}$$\n",
    "\n",
    "Where the degree of the polynomial must be specified additionally to the learning algorithm. When d=1 this is the same as the linear kernel. The polynomial kernel allows for curved lines in the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJWQNcnoCGKZ"
   },
   "source": [
    "#### Gaussian radial basis function (RBF) ####\n",
    "$$k(x_{i}, x_{j})=exp(-\\gamma {\\left \\| x_i-x_j \\right \\|^2})\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWn04erTCGKZ"
   },
   "source": [
    "Implement these kernel functions in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AMrAvicqCGKa"
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: Kernel\n",
    "\n",
    "class Kernel(object):\n",
    "    def linear():\n",
    "        \"\"\"\n",
    "            Returns:\n",
    "            function that takes two vectors as a parameters and returns their dot product\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        return lambda x, y: \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    def polynomial(coef, power):\n",
    "        \"\"\"\n",
    "            Arguments:\n",
    "            coef: float\n",
    "            power: int\n",
    "        \n",
    "            Returns:\n",
    "            function that takes two vectors as a parameters and computes polynomial kernel\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        return lambda x, y: \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    def rbf(gamma):\n",
    "        \"\"\"\n",
    "            Arguments:\n",
    "            gamma: float\n",
    "        \n",
    "            Returns:\n",
    "            function that takes two vectors as a parameters and computes rbf kernel\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (≈ 1-2 line of code)\n",
    "        return lambda x, y: \n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6qwELuymCGKc"
   },
   "outputs": [],
   "source": [
    "lin = Kernel.linear()\n",
    "polynom = Kernel.polynomial(3, 2)\n",
    "rbf = Kernel.rbf(0.0002)\n",
    "\n",
    "xt = x_test[:, 1]\n",
    "yt = x_test[:, 2]\n",
    "\n",
    "print('linear_kernel: ', lin(xt, yt))\n",
    "print('polynomial_kernel: ', polynom(xt, yt))\n",
    "print('rbf_kernel: ', rbf(xt, yt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VeUYwV5PCGKf"
   },
   "source": [
    "**Expected Output**: \n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "      <td><b>linear_kernel:</b></td>\n",
    "    <td> 202.0 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><b>polynomial_kernel:</b></td>\n",
    "    <td>42025.0 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><b>rbf_kernel:</b></td>\n",
    "    <td> 0.9906440418940348 </td> \n",
    "  </tr>\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SVM Algorithm\n",
    "For more deteiled description you may check [this](http://www.math.utep.edu/Faculty/xzeng/2019fall_cps5401/2019fall_cps5401/CPS_5401_Introduction_to_Computational_Sciences_files/CPS5401_SVM.pdf) and [this](https://xavierbourretsicotte.github.io/SVM_implementation.html) articles. Note that designations may vary\n",
    "\n",
    "The lagrangian of our function looks like:\n",
    "\n",
    "$$L = \\sum_{i}(\\lambda) - \\frac{1}{2} \\sum_{i}\\sum_{j}(\\lambda_i*\\lambda_j*y_i*y_j*k(x_i, x_j))$$\n",
    "\n",
    "Where $\\lambda$ - is lagrange multiplier,\n",
    "\n",
    "$y$ - `train_set_y`,\n",
    "\n",
    "$x$ - `train_set_x`,\n",
    "\n",
    "$k()$ - kernel function\n",
    "\n",
    "Maximizing this expression, we will get $\\lambda$ for all the samples. If $\\lambda = 0$ for some samples, this means, that these samples lie deep in it's class, so they can't be support vectors. The set of samples, that can be support vectors will be calles $S$\n",
    "\n",
    "The main dificult in maximization here, that sought variable $\\lambda$ is in second power (means $\\lambda_i*\\lambda_j$). So, in this case, we need to use quadratic optimization.\n",
    "\n",
    "Standart form of quadratic optimization problem is:\n",
    "\n",
    "$$\\min(\\frac{1}{2}x^T \\times P \\times x + q^T \\times x)$$\n",
    "$$s.t.$$\n",
    "$$G \\times x \\leq h$$\n",
    "$$A \\times x = b$$\n",
    "Where sought variable is $x$\n",
    "\n",
    "In our case:\n",
    "\n",
    "$$L = \\sum_{i}(\\lambda) - \\frac{1}{2} \\sum_{i}\\sum_{j}(\\lambda_i*\\lambda_j*y_i*y_j*k(x_i, x_j))$$$$s.t.$$\n",
    "$$\\lambda_i \\geq 0$$\n",
    "$$\\sum_{i}(\\lambda_i*y_i) = 0$$\n",
    "\n",
    "Can be represented in as:\n",
    "\n",
    "$$L = (1,1, ... 1)^T \\times \\lambda - \\frac{1}{2} \\lambda^T \\times y^T \\times y \\times K \\times \\lambda$$\n",
    "$$s.t.$$\n",
    "$$-\\lambda \\leq 0$$\n",
    "$$y * \\lambda = 0$$\n",
    "\n",
    "Where $K$ - is kernel matrix of $X$ (train samples) and $\\lambda$ is sought variable\n",
    "\n",
    "And maximizing of expression above is the same as minimizing of:\n",
    "\n",
    "$$min( \\frac{1}{2} \\lambda^T \\times y^T \\times y \\times K \\times \\lambda - (1,1, ... 1)^T \\times \\lambda)$$\n",
    "$$s.t.$$\n",
    "$$-\\lambda \\leq 0$$\n",
    "$$y * \\lambda = 0$$\n",
    "\n",
    "And now it's looks like standart form of quadratic optimization problem.\n",
    "\n",
    "You just have to define P, q, G, h, A, b and then solve it with `cvxopt.solvers.qp`\n",
    "\n",
    "After getting $\\lambda$ we can replace $\\bar{w}_s$ in our prediction function with:\n",
    "$$\\bar{w}_s = \\sum_{s \\in S}(\\lambda_s * y_s)$$\n",
    "\n",
    "And for making predictions use:\n",
    "$$f(x) = sign(b + \\sum_{s \\in S}(\\lambda_s * y_s*k(x, x_s)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8CW1GtCGKg"
   },
   "source": [
    "### 3.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebPo_0aVCGKg"
   },
   "source": [
    "In the main class you need to implement all the funcionallity:\n",
    "1. `_kernel_matrix` - calculate kernel matrix\n",
    "    - Get number of samples\n",
    "    - Create zero matrix of quadratic shape of number of samples\n",
    "    - Calculate kernels\n",
    "2. `_compute_lagrange_multipliers` - solve a quadratic optimization problem and compute lagrange multipliers\n",
    "    - Get number of samples\n",
    "    - Create Kernel matrix by calling `_kernel_matrix` function\n",
    "    - Create create quadratic term P based on Kernel matrix\n",
    "    - Create linear term q\n",
    "    - Create G, h, A, b\n",
    "    - Solve with - cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    - Return flatten vector of lagrange multipliers\n",
    "3. `_get_support_vectors` - extract support vectors\n",
    "    - Get non-zero lagrange multipliers indicies\n",
    "    - Get he corresponding lagrange multipliers\n",
    "    - Get support vecorts\n",
    "    - Get the samples that will act as support vectors\n",
    "    - Get the corresponding labels\n",
    "4. `fit` - compute b and lagrange multipliers\n",
    "    - Solve a quadratic optimization problem and compute lagrange multipliers by calling `_compute_lagrange_multipliers`\n",
    "    - Extract support vectors and non zero lagrange multipliers by calling `_get_support_vectors`\n",
    "    - Calculate $b$ using first support vector: $b = y_{s0} - \\sum_{s \\in S} \\lambda_s * y_s * k(x_{s0}, x_s)$, where $S$ is a set of all support vectors\n",
    "5. `predict` - use trained by `fit` params to make predictions: $f(x) = sign(b + \\sum_{s \\in S}(\\lambda_s * y_s*k(x, x_s)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5rjAuJ2ZCGKh"
   },
   "outputs": [],
   "source": [
    "#GRADED CLASS: SVM\n",
    "\n",
    "class SVM(object):\n",
    "    \"\"\"\n",
    "    The Support Vector Machines classifier\n",
    "    \n",
    "    Arguments:\n",
    "    C -- penalty term  \n",
    "    kernel -- kernel function e.g. lambda x, y: ...\n",
    "    \"\"\"\n",
    "    def __init__(self, C=1, kernel=Kernel.linear()):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.non_zero_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_labels = None\n",
    "        self.b = None\n",
    "    \n",
    "    def _kernel_matrix(self, X):\n",
    "        \"\"\"\n",
    "        Computes kernel matrix applying kernel function pairwise for each sample\n",
    "\n",
    "        Arguments:\n",
    "        X -- input matrix of shape (number of features, number of samples)\n",
    "        \n",
    "        Returns:\n",
    "        kernels matrix of shape (number of samples, number of samples)\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        # Get number of samples\n",
    "        n_samples = \n",
    "        \n",
    "        # Calculate kernels pairwise and fill kernels matrix\n",
    "        K = \n",
    "\n",
    "        # Return kernel matrix\n",
    "        return K\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def _compute_lagrange_multipliers(self, X, Y):\n",
    "        \"\"\"\n",
    "        Solves the quadratic optimization problem and calculates lagrange multipliers\n",
    "\n",
    "        Arguments:\n",
    "        X -- input matrix of shape (number of features, number of samples)\n",
    "        Y -- labels of shape (1, number of samples)\n",
    "\n",
    "        Returns:\n",
    "        numpy array of lagrange multipliers\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        # Get number of samples\n",
    "        n_samples = \n",
    "        \n",
    "        # Get Kernel matrix by calling _kernel_matrix function\n",
    "        # To create a matrix use cvxopt.matrix\n",
    "        K = \n",
    "\n",
    "        # Create create quadratic term P based on Kernel matrix\n",
    "        # To create a matrix use cvxopt.matrix\n",
    "        P = \n",
    "        \n",
    "        # Create linear term q\n",
    "        # To create a matrix use cvxopt.matrix\n",
    "        q = \n",
    "\n",
    "        # Create G, h\n",
    "        if not self.C:\n",
    "            G = cvxopt.matrix(np.identity(n_samples) * -1)\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            G_max = np.identity(n_samples) * -1\n",
    "            G_min = np.identity(n_samples)\n",
    "            \n",
    "            G = cvxopt.matrix(np.vstack((G_max, G_min)))\n",
    "            \n",
    "            h_max = cvxopt.matrix(np.zeros(n_samples))\n",
    "            h_min = cvxopt.matrix(np.ones(n_samples) * self.C)\n",
    "            \n",
    "            h = cvxopt.matrix(np.vstack((h_max, h_min)))\n",
    "        \n",
    "        # Create A, b\n",
    "        # To create a matrix use cvxopt.matrix\n",
    "        A = \n",
    "        b = \n",
    "\n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        \n",
    "        # Extract flat array of lagrange multipliers\n",
    "        lagrange_multipliers = np.ravel(solution['x'])\n",
    "\n",
    "        # Return lagrange multipliers\n",
    "        return lagrange_multipliers\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "\n",
    "    def _get_support_vectors(self, lagrange_multipliers, X, Y):\n",
    "        \"\"\"\n",
    "        Extracts the samples that will act as support vectors and corresponding labels\n",
    "\n",
    "        Arguments:\n",
    "        lagrange_multipliers -- numpy array of lagrange multipliers\n",
    "        X -- input matrix of shape (number of features, number of samples)\n",
    "        Y -- labels of shape (1, number of samples)\n",
    "\n",
    "        Returns:\n",
    "        non_zero_multipliers -- numpy array of non-zero lagrange multipiers (>1e-7)\n",
    "        support_vectors -- matrix of support vectors of shape (number of features, number of support vectors)\n",
    "        support_vector_labels -- corresponding labels of shape (1, number of support vectors)\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        # Get indexes of non-zero lagrange multipiers\n",
    "        idx = lagrange_multipliers > 1e-7\n",
    "        \n",
    "        # Get the corresponding lagrange multipliers\n",
    "        non_zero_multipliers = \n",
    "        \n",
    "        # Get the samples that will act as support vectors\n",
    "        support_vectors = \n",
    "        \n",
    "        # Get the corresponding labels\n",
    "        support_labels = \n",
    "        \n",
    "        # Return \n",
    "        return non_zero_multipliers, support_vectors, support_labels\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Main training function\n",
    "\n",
    "        Arguments:\n",
    "        X -- input matrix of shape (number of features, number of samples)\n",
    "        Y -- labels of shape (1, number of samples)\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        # Solve the quadratic optimization problem and get lagrange multipliers\n",
    "        lagrange_multipliers = \n",
    "\n",
    "        # Extract support vectors and non zero lagrange multipliers\n",
    "        self.non_zero_multipliers, self.support_vectors, self.support_labels = \n",
    "\n",
    "        # Calculate b using first support vector\n",
    "        self.b = \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict function\n",
    "\n",
    "        Arguments:\n",
    "        X -- input matrix of shape (number of features, number of samples)\n",
    "        \n",
    "        Returns:\n",
    "        predictions of shape (1, number of samples)\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        n_samples = \n",
    "        \n",
    "        predictions = \n",
    "        \n",
    "        return predictions\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlWBQHcLCGKj"
   },
   "source": [
    "Initialize model with default linear term and penalty term = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WwPn-CCKCGKk"
   },
   "outputs": [],
   "source": [
    "model = SVM(C=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOWq1UZuCGKo"
   },
   "source": [
    "Kernel matrix check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BBTeyrmCCGKp"
   },
   "outputs": [],
   "source": [
    "k = model._kernel_matrix(x_test)\n",
    "print('kernel matrix:')\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzpCx9K3CGKr"
   },
   "source": [
    "**Expected Output**: \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "      <td style=\"width:15%\"><b>kernel matrix:</b></td>\n",
    "    <td> [[ 240.  231.  190.  145.  180.]<br>\n",
    " [ 231.  248.  202.  152.  201.]<br>\n",
    " [ 190.  202.  203.  135.  197.]<br>\n",
    " [ 145.  152.  135.  109.  129.]<br>\n",
    " [ 180.  201.  197.  129.  255.]]\n",
    "      </td> \n",
    "  </tr>\n",
    "  \n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmJMFbnmCGKs"
   },
   "source": [
    "Lagrange multipliers check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zLnnaU_HCGKt"
   },
   "outputs": [],
   "source": [
    "lm = model._compute_lagrange_multipliers(x_test, y_test)\n",
    "print('lagrange multipliers: ')\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2aWEWUXCGKv"
   },
   "source": [
    "**Expected Output**: \n",
    "<table style=\"width:30%\">\n",
    "  <tr style=\"width:10%\">\n",
    "      <td><b>lagrange multipliers:</b></td>\n",
    "    <td> [0.00540158<br>0.02154727<br>0.06545855<br>0.05981161<br>0.03259579]\n",
    "      </td> \n",
    "  </tr>\n",
    "  \n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyH1rbe0CGKw"
   },
   "source": [
    "Support vector extraction check, remember that support vectors are columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jDnKniWICGKx"
   },
   "outputs": [],
   "source": [
    "nzl, sv, sl = model._get_support_vectors(lm, x_test,y_test)\n",
    "print('non-zero lagrange multipliers:')\n",
    "print(nzl)\n",
    "print('support vectors:')\n",
    "print(sv)\n",
    "print('support labels:')\n",
    "print(sl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFKzgbYLCGKz"
   },
   "source": [
    "**Expected Output for m_train, m_test**: \n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "    <td><b>non-zero lagrange multipliers:</b></td>\n",
    "    <td> [ 0.00540158<br>  0.02154727<br>  0.06545855<br>  0.05981161<br>  0.03259579]\n",
    "      </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td><b>support vectors:</b></td>\n",
    "    <td> [[5. 5. 2. 2. 2.]<br>\n",
    " [3. 3. 2. 0. 3.]<br>\n",
    " [4. 2. 4. 3. 2.]<br>\n",
    " [1. 1. 0. 0. 0.]<br>\n",
    " [5. 5. 5. 2. 7.]<br>\n",
    " [1. 1. 1. 1. 1.]<br>\n",
    " [0. 0. 1. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 1.]<br>\n",
    " [7. 5. 4. 3. 0.]<br>\n",
    " [1. 1. 1. 0. 1.]<br>\n",
    " [1. 1. 3. 1. 0.]<br>\n",
    " [2. 2. 2. 1. 1.]<br>\n",
    " [2. 2. 0. 1. 1.]<br>\n",
    " [7. 7. 7. 6. 7.]<br>\n",
    " [3. 7. 7. 4. 7.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [2. 2. 2. 2. 2.]<br>\n",
    " [1. 1. 1. 1. 1.]<br>\n",
    " [4. 4. 0. 2. 0.]<br>\n",
    " [3. 2. 3. 1. 7.]<br>\n",
    " [4. 5. 3. 4. 4.]<br>\n",
    " [0. 0. 1. 1. 4.]]\n",
    "      </td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td><b>support labels:</b></td>\n",
    "    <td> [[-1. -1. -1.  1.  1.]]\n",
    "      </td> \n",
    "  </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSiyER-vCGK0"
   },
   "source": [
    "Definition of accuracy metrics for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1xMASksOCGK0"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return np.sum(predictions == labels, axis=1) / float(labels.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tragEwQFCGK2"
   },
   "source": [
    "## 4 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmEDh9zBCGK2"
   },
   "source": [
    "Firstly, let's initialize our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "w975mCf_CGK3"
   },
   "outputs": [],
   "source": [
    "clf = SVM(C=1, kernel=Kernel.linear())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydaaLmGqCGK5"
   },
   "source": [
    "And, finaly, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CcZXpq0DCGK6"
   },
   "outputs": [],
   "source": [
    "clf.fit(train_set_x, train_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yv2Bi7w1CGK9"
   },
   "source": [
    "## 5 - Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bUw4kuNICGK9"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_set_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXuMrR3WCGK_"
   },
   "source": [
    "Let's calculate accuracy (accuracy of model must be >0.97):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_FgVusl6CGLA"
   },
   "outputs": [],
   "source": [
    "accuracy(y_pred, test_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crkEu7-BCGLC"
   },
   "source": [
    "## 6 - Visualization\n",
    "\n",
    "Now let's generate some simple data to see how types of kernel affects the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dRI0xVPfCGLD"
   },
   "outputs": [],
   "source": [
    "samples = np.random.normal(size=200).reshape(2, 100)\n",
    "labels = (2 * (samples.sum(axis=0) > 0) - 1.0).reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZxdK1tyCCGLF"
   },
   "outputs": [],
   "source": [
    "def plot(model, X, Y, grid_size):\n",
    "    \n",
    "    import matplotlib.cm as cm\n",
    "    import itertools\n",
    "    \n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, grid_size),\n",
    "        np.linspace(y_min, y_max, grid_size),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    \n",
    "    flatten = lambda m: np.array(m).reshape(-1,)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    for (i, j) in itertools.product(range(grid_size), range(grid_size)):\n",
    "        point = np.array([[xx[i, j]], [yy[i, j]]])\n",
    "        result.append(model.predict(point)[0, 0])\n",
    "\n",
    "    print(np.array(result).shape)\n",
    "    print(xx.shape)\n",
    "    \n",
    "    Z = np.array(result).reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(\n",
    "        xx, yy, Z,\n",
    "        cmap=cm.Paired,\n",
    "        levels=[-0.01, 0.01],\n",
    "        extend='both',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    \n",
    "    plt.scatter(\n",
    "        flatten(X[0, :]),\n",
    "        flatten(X[1, :]),\n",
    "        c=flatten(Y),\n",
    "        cmap=cm.Paired,\n",
    "    )\n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMRPDFe_CGLH"
   },
   "source": [
    "#### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "s63dfOrxCGLH"
   },
   "outputs": [],
   "source": [
    "clf_lin = SVM(C=1, kernel=Kernel.linear())\n",
    "plot(clf_lin, samples, labels, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1HHtRFBCGLO"
   },
   "source": [
    "#### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Eyo9CuwtCGLP"
   },
   "outputs": [],
   "source": [
    "clf_polynomial = SVM(C=1, kernel=Kernel.polynomial(1, 3))\n",
    "plot(clf_polynomial, samples, labels, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I85NUBFXCGLS"
   },
   "source": [
    "#### RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I-JnwlIvCGLT"
   },
   "outputs": [],
   "source": [
    "clf_rbf = SVM(C=1, kernel=Kernel.rbf(0.03))\n",
    "plot(clf_rbf, samples, labels, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94bJ-18ZCGLb"
   },
   "source": [
    "## 7 - Conclusion\n",
    "As we can see, our model fits well the hypothesis function to the data.\n",
    "\n",
    "#### What's next:\n",
    "1. Try experimenting with the kernel parameters to see how this affects the model you have built.\n",
    "2. Compare the results you have obtained with the `sklearn.svm.SVC` model.\n",
    "3. Try this model in the wild! Select your favorite dataset [here](https://www.kaggle.com/datasets?sortBy=hottest&group=public&page=1&pageSize=20&size=small&filetype=all&license=all&tagids=13303) and play with it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "SVM.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
